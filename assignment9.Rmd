---
title: "Assignment9"
author: "KUN LIU"
date: "2020/11/12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1
## a
Since
$$f(x) = \frac{1}{5\sqrt{2\pi}}x^{2}e^{-\frac{(x-2)^{2}}{2}},g(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}$$
Then $$w(x) = \frac{f(x)}{g(x)} = 0.2x^{2}e^{2x-2}$$

Besides, $h(x) = x^{2}.$
```{r}
set.seed(2)
for (n in c(1000,10000,50000))
{x = rnorm(n)
 mean = mean(0.2*x^4*exp(2*x-2))
 var =  var(0.2*x^4*exp(2*x-2))
 print(paste('when n is',n))
 print(paste('mean is',mean,'variance is',var))
}
```

## b
Since $$h(x)f(x) = \frac{1}{5\sqrt{2\pi}}x^{4}e^{-\frac{(x-2)^{2}}{2}}$$
To find a g(x) that be in proportition to h(x)f(x) as much as possible. I only care the part $x^{4}$ since $e^{-\frac{(x-2)^{2}}{2}}$ is comparely small.

I will choose the pdf of Gamma(5,1) as g(x), where
$$g(x) = \frac{1}{24}x^{4}e^{-x}.$$
Then 
$$h(x)w(x) = \frac{24}{5\sqrt{2\pi}}e^{-\frac{x^{2}-2x+4}{2}}$$

## c
```{r}
set.seed(3)
for (n in c(1000,10000,50000))
{x = rgamma(n,shape = 5, scale = 1)
 temp = 24*exp(-(x^2-6*x+4)/2)/(5*(2*pi)^0.5)
 mean = mean(temp)
 var =  var(temp)
 print(paste('when n is',n))
 print(paste('mean is',mean,'variance is',var))
}

```


## d

Comparing the two results, I find the second method's variances are much more smaller than the first method's. Therefore the second method is a better importance sampling.

# 2

## a
Firstly, 
$$S(T) = S(0)e^{r-\frac{1}{2}\sigma^{2}T+\sigma W(T)}, W(t) = \sqrt{T}Z, Z\sim N(0,1).$$
```{r}
s0 = 1
r = 0.05
m = 12
seed = 1
ST = function(n,t,sigma)
{set.seed(seed)
 z = rnorm(n)
 wt = t**0.5*z
 st = s0*exp((r-0.5*sigma**2)*t+sigma*wt)
 return(st)
}
```
Now, implement the algorithm to simulate 10 paths of S(1) with $\sigma$ = 1.
```{r}
ST(10,1,1)
```


## b
Firstly, define five functions for $P_{A}, P_{E}, P_{G}, S_{A}, S_{G}.$
```{r}
help = function(x)
{return(max(x,0))}

SA = function(n,t,sigma)
{temp = c()
for (i in 1:m)
{temp = cbind(temp,ST(n,i*t/m,sigma))
}
return(apply(temp,1,mean))
}

SG = function(n,t,sigma)
{temp = c()
for (i in 1:m)
{temp = cbind(temp,ST(n,i*t/m,sigma))
}
return(apply(temp,1,prod)^(1/m))
}

PA = function(n,t,sigma,k)
{temp = exp(-r*t)*apply(matrix(SA(n,t,sigma)-k),1,help)
 return(temp)
}

PE = function(n,t,sigma,k)
{temp = exp(-r*t)*apply(matrix(ST(n,t,sigma)-k),1,help)
 return(temp)
}

PG = function(n,t,sigma,k)
{temp = exp(-r*t)*apply(matrix(SG(n,t,sigma)-k),1,help)
 return(temp)
}
```

```{r}
sigma = 0.5
t = 1
n = 5000
for(k in c(1.1,1.2,1.3,1.4,1.5))
{
st = ST(n,t,sigma)
 sa = SA(n,t,sigma)
 sg = SG(n,t,sigma)
 pa = PA(n,t,sigma,k)
 pe = PE(n,t,sigma,k)
 pg = PG(n,t,sigma,k)
 cor1 = cor(pa,st)
 cor2 = cor(pa,pe)
 cor3 = cor(pa,pg)
 print(paste('when k is',k))
 print(paste('the correlations are',cor1,cor2,cor3,'correspondingly'))
}
```
From the above output, I find that when k increases, the correlation coefficient between $P_{A}$ and S(T) decreases obviously, the correlation coefficient between $P_{A}$ and $P_{E}$ decreases a little, the correlation coefficient between $P_{A}$ and $P_{G}$ is almost unchanged.

## c
```{r}
t = 1
k = 1.5
n = 5000
for(sigma in c(0.2,0.3,0.4,0.5))
{
st = ST(n,t,sigma)
 sa = SA(n,t,sigma)
 sg = SG(n,t,sigma)
 pa = PA(n,t,sigma,k)
 pe = PE(n,t,sigma,k)
 pg = PG(n,t,sigma,k)
 cor1 = cor(pa,st)
 cor2 = cor(pa,pe)
 cor3 = cor(pa,pg)
 print(paste('when sigma is',sigma))
 print(paste('the correlations are',cor1,cor2,cor3,'correspondingly'))
}
```
From the above output, I find that when $\sigma$ increases, the correlation coefficient between $P_{A}$ and S(T) increases obviously, the correlation coefficient between $P_{A}$ and $P_{E}$ increases a little, the correlation coefficient between $P_{A}$ and $P_{G}$ is almost unchanged.

## d
```{r}
sigma = 0.5
k = 1.5
n = 5000
for(t in c(0.4,0.7,1,1.3,1.6))
{
st = ST(n,t,sigma)
 sa = SA(n,t,sigma)
 sg = SG(n,t,sigma)
 pa = PA(n,t,sigma,k)
 pe = PE(n,t,sigma,k)
 pg = PG(n,t,sigma,k)
 cor1 = cor(pa,st)
 cor2 = cor(pa,pe)
 cor3 = cor(pa,pg)
 print(paste('when T is',t))
 print(paste('the correlations are',cor1,cor2,cor3,'correspondingly'))
}
```
From the above output, I find that when T increases, the correlation coefficient between $P_{A}$ and S(T) increases obviously, the correlation coefficient between $P_{A}$ and $P_{E}$ increases a little, the correlation coefficient between $P_{A}$ and $P_{G}$ is almost unchanged.

## e
```{r}
sigma = 0.4
t = 1
k = 1.5
n = 500
r = 0.05
s0 = 1
m = 12
Lognorm_expection = function(s0,k,mu,sigma)
{d = (log(s0/k)+mu+sigma^2)/sigma
 temp  = s0*exp(mu+0.5*sigma^2)*pnorm(d)-k*pnorm(d-sigma)
 return(temp)
}
```

Firstly I define a function using control variate.
```{r}
cv = function(n,r,sigma,s0,k,t)
{pa = PA(n,t,sigma,k)
 pg = PG(n,t,sigma,k)
 tbar = (m+1)/(2*m)
 sbar2 = sigma^2/m^2/tbar * sum((2*seq(m)-1)* ((m+1)*t/m - t/m*seq(m)))    
 pg_true_mean = Lognorm_expection(s0,k,(r-0.5*sigma^2)*tbar,sqrt(sbar2*tbar))
 pa_cv_mean = mean(pa) - cov(pg,pa)/var(pg)*(mean(pg)-pg_true_mean)
 return(pa_cv_mean) 
}
```

Then I define a function not using control variate.
```{r}
no_cv = function(n,r,sigma,s0,k,t)
{pa = PA(n,t,sigma,k)
 pa_mean = mean(pa)
 return(pa_mean)
        
}
```

Then I run two functions 200 replicates and compare their estimates' sd.
```{r}
estimator1 = c()
estimator2 = c()

for (i in 1:200)
{seed = i
res1 = cv(n,r,sigma,s0,k,t)
res2 = no_cv(n,r,sigma,s0,k,t)
estimator1 = c(estimator1,res1)
estimator2 = c(estimator2,res2)
}
```

```{r}
sd(estimator1)
sd(estimator2)
```
From the results I find that the SD which has no control variate is larger.
